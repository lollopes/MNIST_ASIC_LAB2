{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c8ebd",
   "metadata": {},
   "source": [
    "# Optimiazion Script using pytorch and tune \n",
    "## Import of libraries\n",
    "It was just added ray respect to previous one. You need to run pip install ray[tune] in your environment for the next cell to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from models.binarized_modules import  BinarizeLinear,BinarizeConv2d\n",
    "from models.binarized_modules import  Binarize,HingeLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13be98b",
   "metadata": {},
   "source": [
    "## Creating the dataloader with a shared directory and the network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets define again the function for binarizing the image \n",
    "\n",
    "class ThresholdTransform(object):\n",
    "    def __init__(self, thr_255):\n",
    "        self.thr = thr_255  \n",
    "\n",
    "    def __call__(self, x):\n",
    "        return (x >= self.thr).to(x.dtype) \n",
    "    \n",
    "#declare the transform  with a shared dir \n",
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)),ThresholdTransform(thr_255=0)])\n",
    "    # Get data from torchvision.datasets\n",
    "    train_data = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "    return train_data,test_data\n",
    "\n",
    "# I will now create a definition of my model as the previous one  \n",
    "class MY_BNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features = 28*28, neurons_l1 = 100, neurons_l2 = 100, neurons_l3 = 100, out_features = 10):\n",
    "        super(MY_BNN, self).__init__()\n",
    "        self.fc1 = BinarizeLinear(in_features, neurons_l1, bias = False)\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        self.bn1 = nn.BatchNorm1d(neurons_l1)\n",
    "        self.fc2 = BinarizeLinear(neurons_l1, neurons_l2, bias = False)\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "        self.bn2 = nn.BatchNorm1d(neurons_l2)\n",
    "        self.fc3 = BinarizeLinear(neurons_l2, neurons_l3, bias = False)\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "        self.bn3 = nn.BatchNorm1d(neurons_l3)\n",
    "        self.fc4 = BinarizeLinear(neurons_l3, out_features, bias = False)\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        self.logsoftmax=nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)    \n",
    "        x = self.bn1(x)    \n",
    "        x = self.htanh1(x) \n",
    "        x = self.fc2(x)    \n",
    "        x = self.bn2(x)    \n",
    "        x = self.htanh2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.htanh3(x)\n",
    "        x = self.fc4(x)\n",
    "        return self.logsoftmax(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985aba6a",
   "metadata": {},
   "source": [
    "## Define a training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bnn(config, checkpoint_dir=None, data_dir=None):\n",
    "    \n",
    "    net = MY_BNN(config[\"neurons_l1\"],config[\"neurons_l2\"],config[\"neurons_l3\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "        os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    trainset, testset = load_data(data_dir)\n",
    "    \n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "    \n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676759e",
   "metadata": {},
   "source": [
    "## Define a test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1878a9",
   "metadata": {},
   "source": [
    "## Define configuration fo search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"neurons_l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"neurons_l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"neurons_l3\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "         #parameter_columns=[\"neurons_l1\", \"neurons_l2\", \"neurons_l1\",\"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_bnn, data_dir=data_dir),\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=10, max_num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b1a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
