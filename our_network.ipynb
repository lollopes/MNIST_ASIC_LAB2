{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcda493",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "In the following block we are just importing the main libraries used for creating a NN and processing its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from models.binarized_modules import  BinarizeLinear,BinarizeConv2d\n",
    "from models.binarized_modules import  Binarize,HingeLoss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85a310",
   "metadata": {},
   "source": [
    "# Load MNIST\n",
    "\n",
    "In the incoming block the MNIST dataset is created and loaded to the standard DataLoader of pytorch. This allow to simply call train_data and test_data when training the network without having to manually create the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76648145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
    "# mean and dividing by std.\n",
    "# We need to recall that the data is normalized when doing the ASIC implementation \n",
    "# The dummy input we feed in the ASIC must be normalized as well \n",
    "\n",
    "#transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Get data from torchvision.datasets\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Define data loaders used to iterate through dataset\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000)\n",
    "\n",
    "# Show some example images and the associated label to verify that the data is loaded correctly \n",
    "\n",
    "labels_map = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9 Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 10, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c37c7",
   "metadata": {},
   "source": [
    "# MNIST classification with Binary Neural Network\n",
    "## Sign function \n",
    "The function implemented below is the sign() function mentioned in the paper. However, this function is not used for training as it would not allow for gradient descent calculation. The idea is to use this function after the train has been performed (TODO). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d75280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sign(a):\n",
    "    \n",
    "    a_buff = torch.empty(a.shape)\n",
    "    for idx, element in enumerate(a):\n",
    "        for idy, sub_element in enumerate(element):\n",
    "            if(sub_element >= 0):\n",
    "                a_buff[idx][idy] = 1\n",
    "            else:\n",
    "                a_buff[idx][idy] = -1\n",
    "            \n",
    "    return a_buff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3809c16",
   "metadata": {},
   "source": [
    "## Create a class for the pytorch BNN\n",
    "Here I am basically creating my own definition of the network, the __init__ is the constructor and creates the class instances of the layer I want to use. The foreward function instead, perform the foreward pass of the network based on the order on which I put the layers previously created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7840a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_BNN(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
    "    pass implemented in forward.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of features in input layer\n",
    "        hidden_dim: number of features in hidden dimension\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(MY_BNN, self).__init__()\n",
    "       \n",
    "        self.fc1 = BinarizeLinear(in_features, hidden_dim)\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = BinarizeLinear(hidden_dim, hidden_dim)\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = BinarizeLinear(hidden_dim, hidden_dim)\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, out_features)\n",
    "        self.logsoftmax=nn.LogSoftmax()\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "       # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        #x = my_sign(x)\n",
    "        x = self.htanh1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        #x = my_sign(x)\n",
    "        x = self.htanh2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.bn3(x)\n",
    "        #x = my_sign(x)\n",
    "        x = self.htanh3(x)\n",
    "        x = self.fc4(x)\n",
    "        return self.logsoftmax(x)\n",
    "        return x\n",
    "    # return self.logsoftmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f218dfe",
   "metadata": {},
   "source": [
    "## Initialize parameters and criterion of the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23336d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pytorch network\n",
    "\n",
    "in_features = 28*28 # this is because the input image is flatten into and array of 28*28, 28 being the number of pixels\n",
    "hidden_dim = 100 # number of neurons in an hidden layer\n",
    "out_features = 10 # we need to classify 10 classes of number 0 to 10 \n",
    "learning_rate = 0.001 # this is the step that we take to move in the direction of the gradient \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Meaning that we use cross entropy as a loss function \n",
    "epochs = 10 # number of times we are going across the full dataset \n",
    "\n",
    "model = MY_BNN(in_features, hidden_dim, out_features)\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate) # Adam algorithm to optimize change of learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e9e8b",
   "metadata": {},
   "source": [
    "## Definition of the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []  # hold the loss for each batch -> used to display training afterwards \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if epoch%40==0:\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.data.copy_(p.org)\n",
    "        optimizer.step()\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.org.copy_(p.data.clamp_(-1,1))\n",
    "                \n",
    "        #print(output)\n",
    "        #print(target)\n",
    "       # correct = torch.argmax(output, axis=1) == torch.argmax(target, axis=1)\n",
    "        #train_accs.append(torch.sum(correct)/len(y_pred))\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede8fe8",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a33305",
   "metadata": {},
   "source": [
    "## Display loss vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for element in train_losses:\n",
    "    new.append(element.detach().numpy())\n",
    "    \n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(new)\n",
    "plt.grid()\n",
    "plt.savefig('foo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77be55e",
   "metadata": {},
   "source": [
    "## Define a function for testing the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdae6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "       \n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62f03c",
   "metadata": {},
   "source": [
    "## Test it\n",
    "After this section the accuracy of the trained network will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687b847",
   "metadata": {},
   "source": [
    "## Test a single prediction after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_data[11]\n",
    "print(label)\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "output = model(img)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6972455",
   "metadata": {},
   "source": [
    "## Printing the weights\n",
    "This section must be refined to make sure that I am able to print the weight for each layer in a proper manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
