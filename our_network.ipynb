{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcda493",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "In the following block we are just importing the main libraries used for creating a NN and processing its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from models.binarized_modules import  BinarizeLinear,BinarizeConv2d\n",
    "from models.binarized_modules import  Binarize,HingeLoss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85a310",
   "metadata": {},
   "source": [
    "# Load MNIST\n",
    "\n",
    "In the incoming block the MNIST dataset is created and loaded to the standard DataLoader of pytorch. This allow to simply call train_data and test_data when training the network without having to manually create the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdTransform(object):\n",
    "    def __init__(self, thr_255):\n",
    "        self.thr = thr_255   # input threshold for [0..255] gray level, convert to [0..1]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return (x >= self.thr).to(x.dtype)  # do not change the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76648145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
    "# mean and dividing by std.\n",
    "# We need to recall that the data is normalized when doing the ASIC implementation \n",
    "# The dummy input we feed in the ASIC must be normalized as well \n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                ThresholdTransform(thr_255=0)])\n",
    "\n",
    "# Get data from torchvision.datasets\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Define data loaders used to iterate through dataset\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000)\n",
    "\n",
    "# Show some example images and the associated label to verify that the data is loaded correctly \n",
    "\n",
    "labels_map = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9 Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 10, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c37c7",
   "metadata": {},
   "source": [
    "# MNIST classification with Binary Neural Network\n",
    "## Sign function \n",
    "The function implemented below is the sign() function mentioned in the paper. However, this function is not used for training as it would not allow for gradient descent calculation. The idea is to use this function after the train has been performed (TODO). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sign(a):\n",
    "    \n",
    "    a_buff = torch.empty(a.shape)\n",
    "    for idx, element in enumerate(a):\n",
    "        for idy, sub_element in enumerate(element):\n",
    "            if(sub_element >= 0):\n",
    "                a_buff[idx][idy] = 1\n",
    "            else:\n",
    "                a_buff[idx][idy] = -1\n",
    "            \n",
    "    return a_buff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50e3a1",
   "metadata": {},
   "source": [
    "## Create a class for the pytorch BNN\n",
    "Here I am basically creating my own definition of the network, the __init__ is the constructor and creates the class instances of the layer I want to use. The foreward function instead, perform the foreward pass of the network based on the order on which I put the layers previously created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7840a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_BNN(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
    "    pass implemented in forward.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of features in input layer\n",
    "        hidden_dim: number of features in hidden dimension\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(MY_BNN, self).__init__()\n",
    "        self.fc1 = BinarizeLinear(in_features, hidden_dim, bias = False)\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = BinarizeLinear(hidden_dim, out_features, bias = False)\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        self.logsoftmax=nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.htanh1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.htanh2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.htanh3(x)\n",
    "        x = self.fc4(x)\n",
    "        return self.logsoftmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f218dfe",
   "metadata": {},
   "source": [
    "## Initialize parameters and criterion of the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23336d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pytorch network\n",
    "\n",
    "in_features = 28*28 # this is because the input image is flatten into and array of 28*28, 28 being the number of pixels\n",
    "hidden_dim = 500 # number of neurons in an hidden layer\n",
    "hidden_layers = 2\n",
    "out_features = 10 # we need to classify 10 classes of number 0 to 10 \n",
    "learning_rate = 0.001 # this is the step that we take to move in the direction of the gradient \n",
    "\n",
    "print(\"Number of bytes required\", (in_features*(hidden_dim**hidden_layers)*out_features)/8/1e6 , \"Mbyte\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Meaning that we use cross entropy as a loss function \n",
    "epochs = 10 # number of times we are going across the full dataset \n",
    "\n",
    "\n",
    "\n",
    "model = MY_BNN(in_features, hidden_dim, out_features)\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate) # Adam algorithm to optimize change of learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e9e8b",
   "metadata": {},
   "source": [
    "## Definition of the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []  # hold the loss for each batch -> used to display training afterwards \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if epoch%40==0:\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.data.copy_(p.org)\n",
    "        optimizer.step()\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.org.copy_(p.data.clamp_(-1,1))\n",
    "                \n",
    "        #print(output)\n",
    "        #print(target)\n",
    "       # correct = torch.argmax(output, axis=1) == torch.argmax(target, axis=1)\n",
    "        #train_accs.append(torch.sum(correct)/len(y_pred))\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.bn2.weight\n",
    "for p in list(model.parameters()):\n",
    "    print(p.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05e517",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58cff9",
   "metadata": {},
   "source": [
    "## Display loss vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc668f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for element in train_losses:\n",
    "    new.append(element.detach().numpy())\n",
    "    \n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(new)\n",
    "plt.grid()\n",
    "plt.savefig('foo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8d5b3",
   "metadata": {},
   "source": [
    "## Define a function for testing the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3980a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "       \n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d1536",
   "metadata": {},
   "source": [
    "## Test it\n",
    "After this section the accuracy of the trained network will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1655f3",
   "metadata": {},
   "source": [
    "## Test a single prediction after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02caedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_data[11]\n",
    "print(label)\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "output = model(img)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf407a",
   "metadata": {},
   "source": [
    "## Printing the weights\n",
    "This section must be refined to make sure that I am able to print the weight for each layer in a proper manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fce913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc2.weight)\n",
    "print(model.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc3.weight)\n",
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc3.weight.shape)\n",
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d069f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc4.weight)\n",
    "print(model.fc4.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.bn1.weight)\n",
    "print(model.fc4.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4ccdb",
   "metadata": {},
   "source": [
    "# Test binarized\n",
    "I know want to test if the network still infers correctly if I have the sign instead of the hardtanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_BNN_test(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
    "    pass implemented in forward.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of features in input layer\n",
    "        hidden_dim: number of features in hidden dimension\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(MY_BNN_test, self).__init__()\n",
    "\n",
    "        self.fc1 = BinarizeLinear(in_features, hidden_dim, bias = False)\n",
    "        self.fc2 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.fc3 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.fc4 = BinarizeLinear(hidden_dim, out_features, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = my_sign(x)\n",
    "        x = self.fc2(x)\n",
    "        x = my_sign(x)\n",
    "        x = self.fc3(x)\n",
    "        x = my_sign(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b42317",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = MY_BNN_test(in_features, hidden_dim, out_features)\n",
    "model_test.fc1.weight = model.fc1.weight\n",
    "model_test.fc2.weight = model.fc2.weight\n",
    "model_test.fc3.weight = model.fc3.weight\n",
    "model_test.fc4.weight = model.fc4.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_test.fc1.weight.shape)\n",
    "print(model_test.fc2.weight.shape)\n",
    "print(model_test.fc3.weight.shape)\n",
    "print(model_test.fc4.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_test():\n",
    "    model_test.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model_test(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabe74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d9d5f",
   "metadata": {},
   "source": [
    "# Testing that outputs are binarized values at each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9dd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a model that prints the output tensor at each stage \n",
    "class MY_BNN_test_printer(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
    "    pass implemented in forward.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of features in input layer\n",
    "        hidden_dim: number of features in hidden dimension\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(MY_BNN_test_printer, self).__init__()\n",
    "\n",
    "        self.fc1 = BinarizeLinear(in_features, hidden_dim, bias = False)\n",
    "        self.fc2 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.fc3 = BinarizeLinear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.fc4 = BinarizeLinear(hidden_dim, out_features, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        print(\"fatten input:\",x,\"shape:\",x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print(\"fc1 output:\",x,\"shape:\",x.shape)\n",
    "        x = my_sign(x)\n",
    "        print(\"sign1 output:\",x,\"shape:\",x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"fc2 output:\",x,\"shape:\",x.shape)\n",
    "        x = my_sign(x)\n",
    "        print(\"sign2 output:\",x,\"shape:\",x.shape)\n",
    "        x = self.fc3(x)\n",
    "        print(\"fc3 output:\",x,\"shape:\",x.shape)\n",
    "        x = my_sign(x)\n",
    "        print(\"sign3 output:\",x,\"shape:\",x.shape)\n",
    "        x = self.fc4(x)\n",
    "        print(\"fc4 output:\",x,\"shape:\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_printer = MY_BNN_test_printer(in_features, hidden_dim, out_features)\n",
    "model_test_printer.fc1.weight = model.fc1.weight\n",
    "model_test_printer.fc2.weight = model.fc2.weight\n",
    "model_test_printer.fc3.weight = model.fc3.weight\n",
    "model_test_printer.fc4.weight = model.fc4.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_test_printer.fc1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56109357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just getting one image to feed in the network\n",
    "\n",
    "img, label = test_data[4]\n",
    "print(label)\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "output = model_test_printer(img)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7507ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
